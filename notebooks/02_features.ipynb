{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4571bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "APPLIC_PATH = PROJECT_ROOT / 'data/raw/application_train.csv'\n",
    "PROCESSED_PATH = PROJECT_ROOT / 'data/processed/application_features_baseline.csv'\n",
    "BUREAU_PATH = PROJECT_ROOT / 'data/raw/bureau.csv'\n",
    "BUREAU_BALANCE_PATH = PROJECT_ROOT / 'data/raw/bureau_balance.csv'\n",
    "\n",
    "INTERIM_PATH = PROJECT_ROOT / 'data/interim'\n",
    "INTERIM_BUREAU = INTERIM_PATH / 'bureau_agg.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fbad55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not APPLIC_PATH.exists() or not BUREAU_PATH.exists() or not BUREAU_BALANCE_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Raw data not found. See README.md for download insturctions.\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(APPLIC_PATH)\n",
    "\n",
    "bureau_df = pd.read_csv(BUREAU_PATH)\n",
    "bureau_balance_df = pd.read_csv(BUREAU_BALANCE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d5af8f",
   "metadata": {},
   "source": [
    "Aggregate bureau balance table and merge it with bureau table by SK_ID_BUREAU key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28fcb38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance_agg = bureau_balance_df.groupby('SK_ID_BUREAU').agg(\n",
    "    loan_duration=('MONTHS_BALANCE', 'count'),\n",
    "    prepaid_ratio=('STATUS', lambda x: (x == 'C').mean()),\n",
    "    default_ever=('STATUS', lambda x: int((x == '5').any())),\n",
    "    bad_dpd_ratio=('STATUS', lambda x: (x.isin(['2', '3', '4', '5']).mean())),\n",
    "    bad_dpd_count=('STATUS', lambda x: (x.isin(['2', '3', '4', '5']).sum())),\n",
    "    small_dpd_ratio=('STATUS', lambda x: (x == '1').mean()),\n",
    "    small_dpd_count=('STATUS', lambda x: (x == '1').sum()),\n",
    "    paid_in_time_ratio=('STATUS', lambda x: (x == '0').mean()),\n",
    "    paid_in_time_count=('STATUS', lambda x: (x == '0').sum()),\n",
    "    unknown_ratio=('STATUS', lambda x: (x == 'X').mean())\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e38d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_df = bureau_df.merge(\n",
    "    bureau_balance_agg, \n",
    "    how='left',\n",
    "    on='SK_ID_BUREAU',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21accce",
   "metadata": {},
   "source": [
    "Aggregate bureau table and merge it with main application table by SK_ID_CURR key. Engineering some features in bureau table before and after merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8c2f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_df['early_closure_days'] = bureau_df['DAYS_CREDIT_ENDDATE'] - bureau_df['DAYS_ENDDATE_FACT']\n",
    "bureau_df['credit_duration'] = abs(bureau_df['DAYS_CREDIT'] - bureau_df['DAYS_ENDDATE_FACT'])\n",
    "\n",
    "bureau_df['cnt_curent_overdue'] = (\n",
    "    (bureau_df['CREDIT_ACTIVE'] == 'Active') & \n",
    "    (bureau_df['CREDIT_DAY_OVERDUE'] > 0)\n",
    ").astype(int)\n",
    "\n",
    "bureau_df['overdue_days_active'] = np.where(\n",
    "    bureau_df['CREDIT_ACTIVE'] == 'Active', \n",
    "    bureau_df['CREDIT_DAY_OVERDUE'],\n",
    "    0\n",
    ")\n",
    "\n",
    "bureau_df['ever_overdue_flag'] = (bureau_df['AMT_CREDIT_MAX_OVERDUE'] > 0).astype(int)\n",
    "\n",
    "bureau_df['overdue_ratio'] = np.where(\n",
    "    bureau_df['AMT_CREDIT_SUM_OVERDUE'] > 0,\n",
    "    bureau_df['AMT_CREDIT_SUM_OVERDUE'] / bureau_df['AMT_CREDIT_SUM'],\n",
    "    0\n",
    ")\n",
    "\n",
    "bureau_df['credit_sum_active'] = np.where(\n",
    "    bureau_df['CREDIT_ACTIVE'] == 'Active',\n",
    "    bureau_df['AMT_CREDIT_SUM'],\n",
    "    0\n",
    ")\n",
    "\n",
    "bureau_df['annuity_active'] = np.where(\n",
    "    bureau_df['CREDIT_ACTIVE'] == 'Active',\n",
    "    bureau_df['AMT_ANNUITY'],\n",
    "    0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "309a9ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_agg = bureau_df.groupby('SK_ID_CURR').agg(\n",
    "    sold_times=('CREDIT_ACTIVE', lambda x: x.isin(['Sold', 'Bad debt']).sum()),\n",
    "    closed_ratio=('CREDIT_ACTIVE', lambda x: (x == 'Closed').mean()),\n",
    "    active_credits=('CREDIT_ACTIVE', lambda x: (x == 'Active').sum()),\n",
    "    first_credit_time=('DAYS_CREDIT', 'min'),\n",
    "    overdue_days_mean=('CREDIT_DAY_OVERDUE', 'mean'),\n",
    "    overdue_days_active_mean=('overdue_days_active', 'mean'),\n",
    "    overdue_active_max=('CREDIT_DAY_OVERDUE', 'max'),\n",
    "    overdue_historical_max=('AMT_CREDIT_MAX_OVERDUE', 'max'),\n",
    "    loans_ever_overdue=('ever_overdue_flag', 'sum'),\n",
    "    loans_overdue_ratio=('ever_overdue_flag', 'mean'),\n",
    "    overdue_credits_active=('cnt_curent_overdue', 'sum'),\n",
    "    overdue_ammount_active=('AMT_CREDIT_SUM_OVERDUE', 'sum'),\n",
    "    overdue_ratio_max=('overdue_ratio', 'max'),\n",
    "    early_closure_days_ratio=('early_closure_days', 'mean'),\n",
    "    credit_duration_mean=('credit_duration', 'mean'),\n",
    "    prolonged_max=('CNT_CREDIT_PROLONG', 'max'),\n",
    "    prolonged_times=('CNT_CREDIT_PROLONG', 'sum'),\n",
    "    credit_sum_mean=('AMT_CREDIT_SUM', 'mean'),\n",
    "    active_credit_sum=('credit_sum_active', 'sum'),\n",
    "    debt_max=('AMT_CREDIT_SUM_DEBT', 'max'),\n",
    "    debt_mean=('AMT_CREDIT_SUM_DEBT', 'mean'),\n",
    "    credit_limit_max=('AMT_CREDIT_SUM_LIMIT', 'max'),\n",
    "    has_credit_card=('CREDIT_TYPE', lambda x: int((x == 'Credit card').any())),\n",
    "    credit_card_cnt=('CREDIT_TYPE', lambda x: (x == 'Credit card').sum()),\n",
    "    low_risk_loans=('CREDIT_TYPE', lambda x: x.isin(['Car loan', 'Mortgage', 'Loan for business development']).sum()),\n",
    "    has_microloan=('CREDIT_TYPE', lambda x: int((x == 'Microloan').any())),\n",
    "    consumer_credit_sum=('CREDIT_TYPE', lambda x: (x == 'Consumer credit').sum()),\n",
    "    last_credit_update=('DAYS_CREDIT_UPDATE', 'max'),\n",
    "    first_credit_update=('DAYS_CREDIT_UPDATE', 'min'),\n",
    "    current_annuity=('annuity_active', 'sum'),\n",
    "    annuity_mean=('AMT_ANNUITY', 'mean'),\n",
    "    loan_duration_avg=('loan_duration', 'mean'),\n",
    "    loan_duration_max=('loan_duration', 'max'),\n",
    "    prepaid_ratio_avg=('prepaid_ratio', 'mean'),\n",
    "    defaults=('default_ever', 'sum'),\n",
    "    worst_dpd=('bad_dpd_ratio', 'max'),\n",
    "    bad_dpd_avg=('bad_dpd_ratio', 'mean'),\n",
    "    bad_dpd_cnt=('bad_dpd_count', 'sum'),\n",
    "    bad_dpd_times_max=('bad_dpd_count', 'max'),\n",
    "    bigest_small_dpd=('small_dpd_ratio', 'max'),\n",
    "    small_dpd_avg=('small_dpd_ratio', 'mean'),\n",
    "    small_dpd_cnt=('small_dpd_count', 'sum'),\n",
    "    paid_in_time_avg=('paid_in_time_ratio', 'mean'),\n",
    "    paid_in_time_cnt=('paid_in_time_count', 'sum'),\n",
    "    unkown_ratio_avg=('unknown_ratio', 'mean')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db40c21",
   "metadata": {},
   "source": [
    "Save aggregated table to interim folder to convinient reuse it in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c4aa8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_agg.to_csv(\n",
    "    INTERIM_BUREAU,\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492be103",
   "metadata": {},
   "source": [
    "Load aggregated table from interim folder to save time due to long aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a4b7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_agg = pd.read_csv(INTERIM_BUREAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae5a3c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(\n",
    "    bureau_agg,\n",
    "    how='left',\n",
    "    on='SK_ID_CURR'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "087bd67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['debt_ratio_mean'] = np.where(\n",
    "   df['credit_sum_mean'] > 0,\n",
    "   df['debt_mean'] / df['credit_sum_mean'],\n",
    "   0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fa4fd6",
   "metadata": {},
   "source": [
    "Stop of bureau feature engineering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd802899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#365243 means NaN in dataset, we add DAYS_EMPLOYED_MISSING as borrowers whith NaN values have lower default rate\n",
    "df['DAYS_EMPLOYED'] = df['DAYS_EMPLOYED'].replace(365243, np.nan)\n",
    "\n",
    "df['days_employed_missing'] = df['DAYS_EMPLOYED'].isna().astype(int)\n",
    "df['DAYS_EMPLOYED'] = df['DAYS_EMPLOYED'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63269058",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['credit_income_ratio'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
    "df['annuity_income_ratio'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cbaf97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothing skewed peaks of data\n",
    "df['log_income'] = np.log1p(df['AMT_INCOME_TOTAL'])\n",
    "df['log_credit'] = np.log1p(df['AMT_CREDIT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f979ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_many_children'] = (df['CNT_CHILDREN'] >= 3).astype(int)\n",
    "\n",
    "# include after baseline\n",
    "# df['cnt_children_capped'] = df['CNT_CHILDREN'].clip(upper=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4f5b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ext_source_1_missing'] = df['EXT_SOURCE_1'].isna().astype(int)\n",
    "df['ext_source_3_missing'] = df['EXT_SOURCE_3'].isna().astype(int)\n",
    "\n",
    "df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bdd6690",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = df.select_dtypes('object').apply(pd.Series.nunique, axis=0)\n",
    "\n",
    "for num, name in zip(cat_features, cat_features.index):\n",
    "    if num <= 2:\n",
    "        df[name] = df[name].astype('category').cat.codes.replace(-1, np.nan)\n",
    "\n",
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f06bc4",
   "metadata": {},
   "source": [
    "replacing binary categories to codes (0, 1), all NaN preserved. One-Hot encoding for all categorical features with more than 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "931ecfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='SK_ID_CURR', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf9d99f",
   "metadata": {},
   "source": [
    "drop useless features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0270d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\n",
    "    PROCESSED_PATH,\n",
    "    index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0130371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_initial_df = pd.read_csv(APPLIC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cee2037",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agg_df = test_initial_df.merge(\n",
    "    bureau_agg,\n",
    "    how='left',\n",
    "    on='SK_ID_CURR'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "802b6a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau_missing_sk_id = test_agg_df.loc[test_agg_df['AMT_REQ_CREDIT_BUREAU_QRT'].notna() & test_agg_df['sold_times'].isna(), 'SK_ID_CURR']\n",
    "bureau_missing_sk_id.isin(bureau_agg['SK_ID_CURR']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c97517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
